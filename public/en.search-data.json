{"/cpu-scheduling/":{"data":{"":" Scheduling Scheduling Algorithms "},"title":"CPU Scheduling"},"/cpu-scheduling/scheduling-algorithms/":{"data":{"":" First Come First Serve Shortest Job First Shortest Remaining Time First Priority Scheduling Round Robin Multilevel Queues Scheduling Multilevel Feedback Queues Scheduling "},"title":"Scheduling Algorithms"},"/cpu-scheduling/scheduling-algorithms/fcfs/":{"data":{"":"","first-come-first-served-fcfs-scheduling#First-Come, First-Served (FCFS) Scheduling":"Definition: FCFS is the simplest CPU scheduling algorithm where the process that arrives first in the ready queue is scheduled first.\nKey Features:\nQueue Type: FIFO (First In, First Out) Type: Non-preemptive Execution Rule: The CPU is assigned to the process at the head of the queue. Drawback: Convoy effect — short processes may wait long times behind longer ones, causing low CPU and device utilization. Waiting Time: Can be high and varies if burst times differ significantly. ","gantt-chart-example#Gantt Chart Example":"Given: Process Arrival Time Burst Time P1 0 5 P2 1 3 P3 2 8 P4 3 6 Assumption All arrive in order: P1 → P2 → P3 → P4 FCFS ignores arrival time if they’re queued. Gantt Chart gantt title FCFS Process Scheduling dateFormat X axisFormat %s section Process Execution P1 (5) :p1, 0, 5 P2 (3) :p2, 5, 8 P3 (8) :p3, 8, 16 P4 (6) :p4, 16, 22 Calculations Process Arrival Time Burst Time Start Time Completion Time Waiting Time Turnaround Time P1 0 5 0 5 0 5 P2 1 3 5 8 4 7 P3 2 8 8 16 6 14 P4 3 6 16 22 13 19 Averages Average Waiting Time = (0 + 4 + 6 + 13) / 4 = 5.75 Average Turnaround Time = (5 + 7 + 14 + 19) / 4 = 11.25 This demonstrates how FCFS can lead to high waiting times especially when short processes follow long ones."},"title":"FCFS"},"/cpu-scheduling/scheduling-algorithms/multilevel-feedback-queues-scheduling/":{"data":{"":"","gantt-chart-example#Gantt Chart Example":"Queue Setup Q0 (Priority 1) — Round Robin, Quantum = 2 Q1 (Priority 2) — Round Robin, Quantum = 4 Q2 (Priority 3) — FCFS Rules New processes enter Q0 If a process uses full quantum in Q0 → move to Q1 If it uses full quantum in Q1 → move to Q2 If it waits too long in Q2 → move up to Q1 (aging) Given: Process Arrival Time Burst Time P1 0 6 P2 1 4 P3 2 9 Assumptions: All processes enter Q0 Apply feedback rules for demotion Execution Breakdown: t=0–2: P1 (Q0) uses full quantum → demote to Q1 t=2–4: P2 (Q0) uses full quantum → demote to Q1 t=4–6: P3 (Q0) uses full quantum → demote to Q1 t=6–10: P1 (Q1, 4 units quantum) → finishes (remaining 4 units) t=10–12: P2 (Q1, remaining 2 units) → finishes t=12–16: P3 (Q1, 4 units) → uses full quantum → demote to Q2 t=16–21: P3 (Q2, remaining 3 units) → finishes Gantt Chart gantt title Multilevel Feedback Queue Scheduling dateFormat X axisFormat %s section Q0 (RR, q=2) P1 (2) :p1a, 0, 2 P2 (2) :p2a, 2, 4 P3 (2) :p3a, 4, 6 section Q1 (RR, q=4) P1 (4) :p1b, 6, 10 P2 (2) :p2b, 10, 12 P3 (4) :p3b, 12, 16 section Q2 (FCFS) P3 (3) :p3c, 16, 19 Calculations Process Arrival Burst Completion Turnaround Waiting P1 0 6 10 10 - 0 = 10 4 P2 1 4 12 12 - 1 = 11 7 P3 2 9 19 19 - 2 = 17 8 Averages Average Waiting Time = (4 + 7 + 8) / 3 = 6.33 Average Turnaround Time = (10 + 11 + 17) / 3 = 12.67 ","multilevel-feedback-queue-scheduling#Multilevel Feedback Queue Scheduling":"Multilevel Feedback Queue Scheduling Definition: Multilevel Feedback Queue Scheduling is an advanced version of Multilevel Queue Scheduling where processes can move between queues based on their behavior and requirements. It aims to dynamically adjust priorities to optimize both CPU-bound and I/O-bound processes.\nKey Features:\nQueue Structure: Multiple queues, each with its own priority level and scheduling algorithm\nDynamic Process Movement:\nDemotion: A process that uses too much CPU time is moved to a lower-priority queue. Promotion: A process that waits too long in a low-priority queue is moved up — this is aging, to prevent starvation. Behavior-Based Scheduling: CPU-bound processes tend to move down I/O-bound or interactive processes stay at higher priorities Adaptivity: Better response time for interactive jobs while ensuring fairness for all Defined By:\nNumber of Queues Scheduling Algorithm per Queue (e.g., RR for top, FCFS for bottom) Rules for Promotion Rules for Demotion Initial Queue Assignment "},"title":"Multilevel Feedback Queues Scheduling"},"/cpu-scheduling/scheduling-algorithms/multilevel-queues-scheduling/":{"data":{"":"","multilevel-queue-scheduling#Multilevel Queue Scheduling":"Multilevel Queue Scheduling Definition: Multilevel Queue Scheduling divides the ready queue into multiple separate queues, each with its own priority level and scheduling algorithm. A process is permanently assigned to one of these queues based on characteristics like memory size, type (interactive/batch), or priority.\nKey Features:\nQueue Structure: Multiple distinct queues (e.g., Foreground, Background)\nProcess Assignment: Permanent to one queue\nScheduling Within Queue: Each queue can have a different scheduling algorithm (e.g., RR for Foreground, FCFS for Background)\nScheduling Among Queues:\nFixed Priority Scheduling: Always schedule from highest priority queue first Time Slicing Among Queues: CPU time split across queues (e.g., 80% to Foreground, 20% to Background) Drawback: Processes in lower-priority queues can suffer starvation\nSolution: Time slicing across queues or aging within queues"},"title":"Multilevel Queues Scheduling"},"/cpu-scheduling/scheduling-algorithms/ps/":{"data":{"":"","gantt-chart-example-non-preemptive-priority#Gantt Chart Example (Non-Preemptive Priority)":"Given: Process Arrival Time Burst Time Priority P1 0 10 3 P2 2 1 1 P3 3 2 4 P4 4 1 2 Execution Trace (Lower number = higher priority)\nt=0 → P1 runs (no other processes yet) t=10 → P2 runs (priority 1) t=11 → P4 runs (priority 2) t=12 → P3 runs (priority 4) Gantt Chart gantt title Priority Scheduling (Non-Preemptive) dateFormat X axisFormat %s section Process Execution P1 (10) :p1, 0, 10 P2 (1) :p2, 10, 11 P4 (1) :p4, 11, 12 P3 (2) :p3, 12, 14 Calculations Process Arrival Time Burst Time Completion Time Turnaround Time Waiting Time P1 0 10 10 10 - 0 = 10 0 P2 2 1 11 11 - 2 = 9 8 P3 3 2 14 14 - 3 = 11 9 P4 4 1 12 12 - 4 = 8 7 Averages Average Waiting Time = (0 + 8 + 9 + 7) / 4 = 6.0 Average Turnaround Time = (10 + 9 + 11 + 8) / 4 = 9.5 ","priority-scheduling#Priority Scheduling":"Priority Scheduling Definition: Priority Scheduling assigns the CPU to the process with the highest priority (lowest numerical value, if lower number means higher priority). It can be preemptive or non-preemptive based on the system’s configuration.\nKey Features:\nType: Can be Preemptive or Non-preemptive Execution Rule: CPU is allocated to the process with the highest priority. Tie-Breaker: FCFS is used if two processes have the same priority. Drawback: May lead to starvation for lower-priority processes. Solution to Starvation: Aging, which increases priority the longer a process waits. "},"title":"PS"},"/cpu-scheduling/scheduling-algorithms/rr/":{"data":{"":"","gantt-chart-example-quantum--4#Gantt Chart Example (Quantum = 4)":"Given: Process Arrival Time Burst Time P1 0 5 P2 1 4 P3 2 2 P4 3 1 Execution Trace (Quantum = 4) t=0 → P1 runs for 4 → remaining = 1 t=4 → P2 runs for 4 → finishes t=8 → P3 runs for 2 → finishes t=10 → P4 runs for 1 → finishes t=11 → P1 runs for 1 → finishes Gantt Chart gantt title Round Robin Scheduling (Quantum = 4) dateFormat X axisFormat %s section Process Execution P1 (4) :p1a, 0, 4 P2 (4) :p2, 4, 8 P3 (2) :p3, 8, 10 P4 (1) :p4, 10, 11 P1 (1) :p1b, 11, 12 Calculations Process Arrival Time Burst Time Completion Time Turnaround Time Waiting Time P1 0 5 12 12 - 0 = 12 12 - 5 = 7 P2 1 4 8 8 - 1 = 7 3 P3 2 2 10 10 - 2 = 8 6 P4 3 1 11 11 - 3 = 8 7 Averages Average Waiting Time = (7 + 3 + 6 + 7) / 4 = 5.75 Average Turnaround Time = (12 + 7 + 8 + 8) / 4 = 8.75 This example shows that Round Robin gives each process a fair chance to execute, but context switches and response times can vary depending on the quantum and arrival patterns.","round-robin-rr-scheduling#Round Robin (RR) Scheduling":"Round Robin (RR) Scheduling Definition: Round Robin is a preemptive CPU scheduling algorithm where each process is assigned a fixed time quantum. Processes are scheduled in a circular order. If a process’s burst time exceeds the quantum, it is preempted and moved to the end of the ready queue.\nKey Features:\nType: Preemptive Execution Rule: Each process gets the CPU for at most one time quantum. Queue Type: Circular FIFO Fairness: Every process gets an equal share of the CPU over time. Preemption: Occurs after quantum expires. Drawback: High context switching overhead if quantum is too small. "},"title":"RR"},"/cpu-scheduling/scheduling-algorithms/sjf/":{"data":{"":"","gantt-chart-example-non-preemptive-sjf#Gantt Chart Example (Non-Preemptive SJF)":"Given: Process Arrival Time Burst Time P1 0 6 P2 1 8 P3 2 7 P4 3 3 Execution Order (Non-Preemptive SJF) At time 0 → only P1 is available → run P1 At time 6 → P2, P3, P4 are available → choose shortest: P4 At time 9 → P2, P3 are available → choose P3 At time 16 → run P2\nGantt Chart gantt title Non-Preemptive SJF Process Scheduling dateFormat X axisFormat %s section Process Execution P1 (6) :p1, 0, 6 P4 (3) :p4, 6, 9 P3 (7) :p3, 9, 16 P2 (8) :p2, 16, 24 Calculations Process Arrival Time Burst Time Start Time Completion Time Waiting Time Turnaround Time P1 0 6 0 6 0 6 P2 1 8 16 24 15 23 P3 2 7 9 16 7 14 P4 3 3 6 9 3 6 Averages Average Waiting Time = (0 + 15 + 7 + 3) / 4 = 6.25 Average Turnaround Time = (6 + 23 + 14 + 6) / 4 = 12.25 ","shortest-job-first-sjf-scheduling#Shortest Job First (SJF) Scheduling":"Shortest Job First (SJF) Scheduling Definition: SJF is a CPU scheduling algorithm where the process with the smallest CPU burst time is scheduled next.\nKey Features:\nType: Can be Non-preemptive or Preemptive (Shortest Remaining Time First - SRTF) Execution Rule: CPU is assigned to the process with the shortest next CPU burst. Tie-Breaker: If two processes have the same burst time, FCFS is used. Difficulty: Actual next CPU burst time is unknown, must be predicted (e.g., using exponential averaging). Drawback: Potential for starvation of longer jobs. "},"title":"SJF"},"/cpu-scheduling/scheduling-algorithms/srtf/":{"data":{"":"","gantt-chart-example-srtf#Gantt Chart Example (SRTF)":"Given: Process Arrival Time Burst Time P1 0 8 P2 1 4 P3 2 9 P4 3 5 Execution Trace t=0 → P1 arrives → run P1 t=1 → P2 arrives (4 \u003c 7) → preempt P1, run P2 t=2 → P3 arrives (9 \u003e 3) → continue P2 t=3 → P4 arrives (5 \u003e 2) → continue P2 t=5 → P2 finishes → pick shortest: P4 (5), P1 (7), P3 (9) → run P4 t=10 → P4 finishes → pick shortest: P1 (7), P3 (9) → run P1 t=17 → P1 finishes → run P3 t=26 → P3 finishes Gantt Chart gantt title SRTF (Preemptive SJF) Process Scheduling dateFormat X axisFormat %s section Process Execution P1 (start) :p1a, 0, 1 P2 (full) :p2, 1, 5 P4 (full) :p4, 5, 10 P1 (rest) :p1b, 10, 17 P3 (full) :p3, 17, 26 Calculations Process Arrival Time Burst Time Completion Time Turnaround Time Waiting Time P1 0 8 17 17 - 0 = 17 17 - 8 = 9 P2 1 4 5 5 - 1 = 4 0 P3 2 9 26 26 - 2 = 24 15 P4 3 5 10 10 - 3 = 7 2 Averages Average Waiting Time = (9 + 0 + 15 + 2) / 4 = 6.5 Average Turnaround Time = (17 + 4 + 24 + 7) / 4 = 13.0 This example demonstrates how SRTF aggressively preempts longer processes for shorter ones, reducing turnaround for shorter jobs but potentially increasing waiting for longer ones.","shortest-remaining-time-first-srtf-scheduling#Shortest Remaining Time First (SRTF) Scheduling":"Shortest Remaining Time First (SRTF) Scheduling Definition: SRTF is the preemptive version of Shortest Job First (SJF), where the process with the least remaining CPU burst time is executed next. If a new process arrives with a shorter remaining time than the currently executing one, preemption occurs.\nKey Features:\nType: Preemptive Execution Rule: Always run the process with the shortest remaining time. Tie-Breaker: If two processes have the same remaining time, FCFS is used. Preemption: Occurs when a new process has less remaining time than the current process. Drawback: Longer processes may suffer from starvation. Estimation Issue: Requires knowledge or prediction of the remaining CPU time. "},"title":"SRTF"},"/cpu-scheduling/scheduling/":{"data":{"":"","cpu-scheduler#CPU Scheduler":"CPU Scheduler When the CPU becomes idle, the operating system must select one of the processes in the ready queue to execute next. This selection is done by the short-term scheduler, also called the CPU scheduler. It picks a process from memory that is ready to run and calls the dispatcher to allocate the CPU to that process. The ready queue may be implemented using different data structures like FIFO queues, trees, or unordered linked lists. Each entry in the ready queue is usually a process control block (PCB) representing a process.","dispatcher#Dispatcher":"The dispatcher is a part of the kernel responsible for switching the CPU from the currently running process to the one chosen by the CPU scheduler. This involves three key tasks:\nSaving the context of the current process and restoring the context of the new process Switching from kernel mode to user mode Jumping to the appropriate instruction in the new process to resume execution The total time required to perform this switch is known as dispatch latency.\nsequenceDiagram participant CPU participant Process A participant Scheduler participant Process B Process A-\u003e\u003eCPU: Currently running CPU-\u003e\u003eScheduler: Becomes idle or interrupted Scheduler-\u003e\u003eProcess B: Selects next process Scheduler-\u003e\u003eCPU: Calls dispatcher CPU-\u003e\u003eProcess A: Saves context CPU-\u003e\u003eProcess B: Restores context and runs ","preemptive-and-non-preemptive-scheduling#Preemptive and Non-Preemptive Scheduling":"Scheduling can be either preemptive or non-preemptive depending on when the CPU is taken away from a process.\nIf a process moves from running to waiting (for example, during an I/O request) or terminates, the scheduler makes a decision without forcing an interruption. This is non-preemptive scheduling.\nIf a process moves from running to ready (due to an interrupt) or from waiting to ready (for example, I/O completion), then the currently running process may be interrupted to allow another to run. This is preemptive scheduling.","scheduling-criteria#Scheduling Criteria":"CPU utilization measures how effectively the CPU is being used. The goal is to keep it busy as much as possible. In real systems, CPU utilization ranges from 40% on lightly loaded systems to 90% in heavily used environments.\nThroughput refers to the number of processes completed per unit of time. A higher throughput indicates more work is being done, and is desirable.\nTurnaround time is the total time from when a process is submitted to when it is completed. It includes time spent in memory allocation, waiting in the ready queue, executing on the CPU, and performing I/O. Minimizing turnaround time improves system responsiveness.\nWaiting time is the amount of time a process spends waiting in the ready queue before execution. Reducing waiting time increases CPU efficiency and ensures quicker processing of tasks.\nResponse time is the interval between the submission of a request and the system’s first response. It does not include the time taken to complete the request, only the time to begin processing. Lower response time makes the system appear faster to users"},"title":"Scheduling"},"/deadlock/":{"data":{"":" Deadlock Problem Resource Allocation Graphs Deadlock Handling "},"title":"Deadlock"},"/deadlock/deadlock-handling/":{"data":{"":" Deadlock Prevention Dealock Avoidance Dealock Detection Dealock Recovery "},"title":"Deadlock Handling"},"/deadlock/deadlock-problem/":{"data":{"":"","concept#Concept":"Concept In a computer system, deadlock refers to a situation where a set of processes become stuck, each waiting for a resource that is currently held by another process in the same set. As a result, none of them can proceed, and unless something external intervenes (like manual killing of a process), the system remains in this stalled state indefinitely.","conditions-for-deadlock#Conditions for Deadlock":"A deadlock can occur only if all the following four conditions are true at the same time in the system. These are known as the Coffman conditions:\n1. Mutual Exclusion Some resources cannot be shared. If one process is using a resource (like a printer), others must wait.\nThis is essential for correctness (e.g., two processes can’t print on the same printer at the same time), but it also introduces exclusivity — a precondition for deadlock.\n2. Hold and Wait A process is holding one or more resources and is waiting to acquire additional resources that are currently held by other processes.\nFor example, a process may have a lock on a file and then request access to the printer. If the printer is unavailable, the process will wait — while still holding the file lock.\n3. No Preemption Resources cannot be forcibly taken from a process. A resource can only be released voluntarily by the process that holds it after it has completed its task.\nThis means that if a process is holding a resource, no other process or even the OS can take it away — it must wait.\n4. Circular Wait There exists a cycle of waiting processes: each process in the cycle is waiting for a resource that the next process in the cycle holds.\nExample:\nP1 waits for a resource held by P2 P2 waits for a resource held by P3 … Pn waits for a resource held by P1 This closed loop creates a situation where no process can proceed, because each is waiting for another.","example-with-semaphores#Example with Semaphores":"Consider two semaphores A and B, both initialized to 1 (i.e., available). Two processes, P1 and P2, try to acquire them:\nProcess P1:\nwait(A); wait(B); Process P2:\nwait(B); wait(A); Here’s what could happen:\nP1 gets semaphore A and is now holding it. Simultaneously, P2 gets semaphore B and is now holding it. P1 now wants B, but it’s held by P2 → P1 is blocked. P2 now wants A, but it’s held by P1 → P2 is blocked. Both are now blocked forever. This is a deadlock.","the-system-model-for-deadlocks#The System Model for Deadlocks":"A computer system consists of a set of resources that are shared among processes. These resources can include:\nCPU cycles Memory Disk drives I/O devices (like printers or network cards) Each resource type may have one or more instances. For example, there may be multiple disk drives or only one printer.\nProcesses use resources through the following sequence:\nRequest: The process asks for the resource. Use: The process does work with the resource. Release: The process releases the resource. If a resource is not available when requested, the process goes into a waiting state until the resource becomes available."},"title":"Deadlock Problem"},"/deadlock/resource-allocation-graphs/":{"data":{"":"","case-1-no-cycle#Case 1: No Cycle":" graph LR P1((P1)) --\u003e R1((R1)) R1 --\u003e P2((P2)) P2 --\u003e R2((R2)) R2 --\u003e P3((P3)) Explanation P1 is requesting R1 R1 is assigned to P2 P2 is requesting R2 R2 is assigned to P3 There is no cycle in the graph — hence, no deadlock","case-2-cycle-with-1-resource-instance#Case 2: Cycle with 1 Resource Instance":" graph LR P1((P1)) --\u003e R1((R1)) R1 --\u003e P2((P2)) P2 --\u003e R2((R2)) R2 --\u003e P1 Explanation P1 requests R1 R1 is assigned to P2 P2 requests R2 R2 is assigned to P1 There is a cycle: P1 → R1 → P2 → R2 → P1\nIf each resource type (R1, R2) has only one instance, this cycle implies a deadlock","case-3-possibility-of-deadlock#Case 3: Possibility of Deadlock":" graph LR P1((P1)) --\u003e R1((R1)) R1 --\u003e P2((P2)) P2 --\u003e R1 R1 --\u003e P3((P3)) Explanation P1 and P2 both request R1. R1 assigns one instance to P2 and one to P3. Even though there’s a cycle (P2 → R1 → P3 → R1 → P2), since R1 has multiple instances, this is not necessarily a deadlock With multiple instances, cycles may or may not indicate deadlock","concept#Concept":"Concept A Resource Allocation Graph models the state of resource usage in a system.\nThe graph consists of:\nProcesses (P1, P2, …, Pn) Resource Types (R1, R2, …, Rm) There are two types of edges:\nRequest edge: From a process to a resource (e.g., P1 --\u003e R1) Assignment edge: From a resource to a process (e.g., R1 --\u003e P1) "},"title":"Resource Allocation Graphs"},"/exec-family/":{"data":{"":"","execl#execl()":"Replaces current process with a new one using a path and a list of arguments\nint execl(const char *path, const char *arg, ..); Return Values:\nSuccess: No return Error: -1 Example:\nexecl(\"/bin/ls\", \"ls\", \"-l\", NULL); ","execle#execle()":"Like execl() but allows specifying a custom environment\nint execle(const char *path, const char *arg, .., char * const envp[]); Return Values:\nSuccess: No return Error: -1 Example:\nchar *env[] = { \"MYVAR=VALUE\", NULL }; execle(\"/bin/ls\", \"ls\", \"-l\", NULL, env); ","execlp#execlp()":"Like execl() but searches PATH for the executable\nint execlp(const char *file, const char *arg, ..); Return Values:\nSuccess: No return Error: -1 Example:\nexeclp(\"ls\", \"ls\", \"-l\", NULL); ","execv#execv()":"Replaces current process using a path and an argument vector (array)\nint execv(const char *path, char *const argv[]); Return Values:\nSuccess: No return Error: -1 Example:\nchar *args[] = { \"ls\", \"-l\", NULL }; execv(\"/bin/ls\", args); ","execvp#execvp()":"Like execv() but searches PATH for the executable\nint execvp(const char *file, char *const argv[]); Return Values:\nSuccess: No return Error: -1 Example:\nchar *args[] = { \"ls\", \"-l\", NULL }; execvp(\"ls\", args); ","execvpe#execvpe()":"Like execvp() but also allows specifying a custom environment\nint execvpe(const char *file, char *const argv[], char *const envp[]); Return Values:\nSuccess: No return Error: -1 Example:\nchar *args[] = { \"ls\", \"-l\", NULL }; char *env[] = { \"MYVAR=VALUE\", NULL }; execvpe(\"ls\", args, env); ","library-required#Library Required":"Library Required #include \u003cunistd.h\u003e "},"title":"Exec Family"},"/file-operations/":{"data":{"":"","close#close()":"Closes an open file descriptor.\nint close(int fd); Retrun Value:\nSuccess: 0 Error: -1 Example:\nclose(fd); ","dup#dup()":"Duplicates an existing file descriptor to the lowest*numbered unused one.\nint dup(int oldfd); Retrun Value:\nSuccess: New file descriptor Error: -1 Example:\nint dup_fd = dup(fd); // dup_fd now refers to the same file ","dup2#dup2()":"Duplicates a file descriptor to a specified descriptor number, closing it first if needed.\nint dup2(int oldfd, int newfd); Retrun Value:\nSuccess: New file descriptor Error: -1 Example:\ndup2(fd, STDOUT_FILENO); // redirect standard output to fd ","file-reading-functions#File Reading Functions":"fopen() Opens a file and returns a file pointer to it.\nFILE *fopen(const char *filename, const char *mode); Return Value:\nSuccess: Pointer to FILE Error: NULL Example:\nFILE *fp = fopen(\"example.txt\", \"r\"); fclose() Closes a previously opened file stream.\nint fclose(FILE *stream); n Return Value:\nSuccess: 0 Error: EOF Example:\nfclose(fp); fgets() Reads a string from a file, stopping at newline or EOF.\nchar *fgets(char *str, int n, FILE *stream); Return Value:\nSuccess: str Error or EOF: NULL Example:\nchar line[256]; fgets(line, sizeof line, fp); fgetc() Reads a single character from a file.\nint fgetc(FILE *stream); Return Value:\nSuccess: Character as an unsigned char cast to int Error or EOF: EOF Example:\nint ch = fgetc(fp); // returns int, not char fread() Reads binary data from a file stream.\nsize_t fread(void *ptr, size_t size, size_t count, FILE *stream); Return Value:\nNumber of full elements successfully read Example:\nint block[512]; size_t n = fread(block, 1, sizeof block, fp); fscanf() Reads formatted input from a file stream.\nint fscanf(FILE *stream, const char *format, ...); Return Value:\nNumber of items successfully read and assigned EOF if input failure occurs before any conversion Example:\nint x, y; fscanf(fp, \"%d %d\", \u0026x, \u0026y); feof() Checks whether the end-of-file indicator is set.\nint feof(FILE *stream); Return Value:\nNon-zero if EOF indicator is set 0 otherwise Example:\nif (feof(fp)) { /* reached end-of-file */ } ferror() Checks whether a file stream has an error.\nint ferror(FILE *stream); Return Value:\nNon-zero if an error occurred 0 otherwise Example:\nif (ferror(fp)) { /* handle stream error */ } ","file-writing-functions#File Writing Functions":"fprintf() Writes formatted output to a file stream.\nint fprintf(FILE *stream, const char *format, ...); Return Value:\nNumber of characters printed Negative number on error Example:\nfprintf(fp, \"Value = %d\\n\", 42); fputs() Writes a null-terminated string to a file.\nint fputs(const char *str, FILE *stream); Return Value:\nSuccess: Non-negative value Error: EOF Example:\nfputs(\"Line of text\\n\", fp); fputc() Writes a single character to a file.\nint fputc(int c, FILE *stream); Return Value:\nSuccess: Character written (as unsigned char cast to int) Error: EOF Example:\nfputc('A', fp); fwrite() Writes binary data to a file stream.\nsize_t fwrite(const void *ptr, size_t size, size_t count, FILE *stream); Return Value:\nNumber of full elements successfully written Example:\nfwrite(block, 1, sizeof block, fp); fflush() Flushes a file’s output buffer to disk.\nint fflush(FILE *stream); Return Value:\nSuccess: 0 Error: EOF Example:\nfflush(fp); // force buffered output to disk ","libraries-required#Libraries Required":"Libraries Required #include \u003cfcntl.h\u003e #include \u003cunistd.h\u003e ","open#open()":"Opens a file and returns a file descriptor for it.\nint open(const char *pathname, int flags, mode_t mode); Retrun Value:\nSuccess: File descriptor Error: -1 Example:\nint fd = open(\"example.txt\", O_RDONLY); ","printf#printf()":"Writes formatted output to standard output.\nint printf(const char *format, ...); Return Value:\nNumber of characters printed Negative value if an output error occurs Examples:\n// Printing an integer int x = 10; printf(\"x = %d\\n\", x); // Printing a float float f = 3.14; printf(\"f = %.2f\\n\", f); // Printing a character char c = 'A'; printf(\"Character: %c\\n\", c); // Printing a string char str[] = \"Hello\"; printf(\"String: %s\\n\", str); // Printing multiple values int id = 101; char grade = 'A'; float marks = 89.5; printf(\"ID: %d, Grade: %c, Marks: %.1f\\n\", id, grade, marks); // Printing hexadecimal and octal int num = 255; printf(\"Hex: %x, Octal: %o\\n\", num, num); // Printing a pointer address int *ptr = \u0026x; printf(\"Address: %p\\n\", ptr); ","read#read()":"Reads data from a file descriptor into a buffer\nssize_t read(int fd, void *buf, size_t count); Retrun Value:\nSuccess: Number of bytes read Error: -1 Example:\nchar buf[128]; ssize_t nread = read(fd, buf, sizeof buf); ","scanf#scanf()":"Reads formatted input from standard input.\nint scanf(const char *format, ...); Return Value:\nNumber of items successfully read and assigned EOF if input failure occurs before any conversion Examples:\n// Reading an integer int x; scanf(\"%d\", \u0026x); // Reading a float float f; scanf(\"%f\", \u0026f); // Reading a character char c; scanf(\" %c\", \u0026c); // space before %c to skip whitespace // Reading a string char str[100]; scanf(\"%s\", str); // stops at first whitespace // Reading multiple values int a, b; scanf(\"%d %d\", \u0026a, \u0026b); // Reading a double double d; scanf(\"%lf\", \u0026d); // Reading formatted values into different types int id; char grade; float marks; scanf(\"%d %c %f\", \u0026id, \u0026grade, \u0026marks); ","types#Types":" size_t // is a unsigned integer type used to represent the number of bytes read or written ssize_t // is a signed integer type used to represent the number of bytes read or written FILE* // pointer to struct that represent file stream ","write#write()":"Writes data from a buffer to a file descriptor.\nssize_t write(int fd, const void *buf, size_t count); Retrun Value:\nSuccess: Number of bytes written Error: -1 Example:\nconst char *msg = \"Hello\\n\"; ssize_t nwritten = write(fd, msg, strlen(msg)); "},"title":"File Operations"},"/pipes--fifos/":{"data":{"":"","libraries-required#Libraries Required":"Libraries Required #include \u003cunistd.h\u003e #include \u003csys/stat.h\u003e ","mkfifo#mkfifo()":"Creates a named pipe (FIFO) special file that can be used for inter-process communication\nint mkfifo(const char *pathname, mode_t mode); Return Value:\nSuccess: 0 Error: -1 Modes:\nOctal Symbolic Description 0400 r– — — Read by owner 0200 -w- — — Write by owner 0600 rw- — — Read/Write by owner 0040 — r– — Read by group 0020 — -w- — Write by group 0060 — rw- — Read/Write by group 0004 — — r– Read by others 0002 — — -w- Write by others 0006 — — rw- Read/Write by others 0666 rw- rw- rw- Read/Write by all (owner/group/others) 0644 rw- r– r– Owner RW, Group R, Others R Tip\nOwner: The user who created the file and has primary control over it Group: Set of different users Others: Users neither owner nor group Example:\nmkfifo(\"mypipe\", 0666); int fd = open(\"mypipe\", O_WRONLY); write(fd, \"Hi\", 2); ","pipe#pipe()":"Creates a unidirectional data channel (pipe) using a pair of file descriptors for reading and writing\nint pipe(int pipefd[2]); Return Value:\nSuccess: 0 Error: -1 Example:\nint fd[2]; pipe(fd); if (fork() == 0) { close(fd[0]); write(fd[1], \"Hello\", 5); _exit(0); } else { close(fd[1]); char buf[6] = {0}; read(fd[0], buf, 5); printf(\"Parent read: %s\\n\", buf); } "},"title":"Pipes \u0026 FIFOs"},"/process-control/":{"data":{"":"","_exit#_exit()":"Immediately terminates the process without flushing stdio buffers or calling cleanup handlers.\nvoid _exit(int status); Return Value:\nDoes not return Example:\n_exit(0); ","exit#exit()":"Terminates the calling process and performs cleanup (e.g., flushes stdio buffers).\nvoid exit(int status); Return Value:\nDoes not return Example:\nexit(0); ","fork#fork()":"fork() Creates a new child process by duplicating the calling (parent) process.\npid_t fork(void); Return Value:\nParent: child’s PID (positive) Child: 0 Error: -1 Example:\npid_t pid = fork(); if (pid == 0) { printf(\"Child process\\n\"); _exit(0); } else { printf(\"Parent process, child PID: %d\\n\", pid); } ","macros#MACROS":"WIFEXITED Returns true if the child terminated normally (via exit() or _exit()).\nWIFEXITED(status) Return Value:\nTrue if child terminated normally Example:\nint status; wait(\u0026status); if (WIFEXITED(status)) { printf(\"Child exited normally\\n\"); } WEXITSTATUS Returns the exit status code of the child (only valid if WIFEXITED(status) is true).\nWEXITSTATUS(status) Return Value:\nReturn code when WIFEXITED is true Example:\nif (WIFEXITED(status)) { int code = WEXITSTATUS(status); printf(\"Exit code: %d\\n\", code); } WIFSIGNALED Returns true if the child terminated due to an uncaught signal.\nWIFSIGNALED(status) Return Value:\nTrue if child terminated by signal Example:\nif (WIFSIGNALED(status)) { printf(\"Child killed by signal\\n\"); } ","wait#wait()":"Suspends execution of the calling process until any of its child processes terminates.\npid_t wait(int *status); Return Value:\nSuccess: PID of terminated child Error: -1 Example:\nint status; wait(\u0026status); ","waitpid#waitpid()":"Waits for a specific child process or set of children to terminate.\npid_t waitpid(pid_t pid, int *status, int options); Return Value:\nSuccess: PID of child Error: -1 With WNOHANG and no children ready: 0 Example:\nint status; pid_t child = fork(); if (child == 0) _exit(5); waitpid(child, \u0026status, 0); "},"title":"Process Control"},"/process-synchronization/":{"data":{"":" Shared Memory Producer Consumer Problem Critical Section Problem Semaphores Bounded Buffer Problem Reader Writer Problem Dining Philosipher Problem Sleeping Barber Problem "},"title":"Process Synchronization"},"/process-synchronization/bounded-buffer-problem/":{"data":{"":"","correctness-achieved#Correctness Achieved":" Requirement Mechanism Mutual Exclusion mutex protects critical sections. No Overflow Producer proceeds only when empty \u003e 0. No Underflow Consumer proceeds only when full \u003e 0. Progress Atomic semaphore operations prevent deadlock; whichever process has its condition satisfied will continue. Bounded Waiting Each wait queues callers FIFO; each signal awakens one, ensuring finite wait time relative to buffer capacity. This semaphore-based solution allows the producer and consumer to run concurrently without race conditions, while respecting the bounded size of the shared buffer.","how-the-semaphores-coordinate-the-processes#How the Semaphores Coordinate the Processes":" Step Semaphore Action Effect Space check wait(empty) (Producer) Blocks if buffer is full; otherwise reserves one slot. Data check wait(full) (Consumer) Blocks if buffer is empty; otherwise reserves one item. Mutual exclusion wait(mutex) / signal(mutex) Only one process at a time can modify buffer, in, or out. Post-operation signal signal(full) (Producer) Announces a new item is available. signal(empty) (Consumer) Announces a slot has been freed. ","problem-statement#Problem Statement":"Problem Statement Two concurrent processes share a fixed-size circular buffer:\nRole Action Producer Generates items and places them into the buffer. Consumer Removes items from the buffer for use. Because the buffer is bounded, the producer must wait when it is full, and the consumer must wait when it is empty. Correctness requires mutual exclusion while either process accesses the shared indices or buffer slots.","solution#Solution":"Synchronization Primitives Semaphore Initial Value Meaning mutex 1 Binary semaphore ensuring mutual exclusion for buffer access. empty N Counting semaphore tracking free slots (N = buffer size). full 0 Counting semaphore tracking filled slots. All semaphore operations (wait, signal) are atomic.\nShared Data #define N 10 item buffer[N]; int in = 0; // next free position int out = 0; // next filled position semaphore mutex = 1; // mutual exclusion semaphore empty = N; // initially N slots are empty semaphore full = 0; // initially 0 items are full Producer while (true) { item x = produce_item(); // generate item wait(empty); // decrement empty count (block if 0) wait(mutex); // enter critical section buffer[in] = x; // insert item in = (in + 1) % N; // advance index circularly signal(mutex); // leave critical section signal(full); // increment full count } Consumer while (true) { wait(full); // decrement full count (block if 0) wait(mutex); // enter critical section item y = buffer[out]; // remove item out = (out + 1) % N; // advance index circularly signal(mutex); // leave critical section signal(empty); // increment empty count consume_item(y); // use item } "},"title":"Bounded Buffer Problem"},"/process-synchronization/critical-section-problem/":{"data":{"":"","n-process-solution--bakery-algorithm#N-Process Solution – \u003cstrong\u003eBakery Algorithm\u003c/strong\u003e":"Problem Statement When several cooperating processes access the same shared data, any segment of code that touches that data is called a critical section. A correct solution must ensure:\nRequirement Meaning Mutual Exclusion At most one process can be inside its critical section at any instant. Progress If no one is in a critical section, the decision of which process enters next cannot be postponed by processes that are only executing in their remainder sections. Bounded Waiting After a process requests entry, there is a finite bound on how many times others may enter before it gets its turn. Assumptions for classic software proofs: each process executes at a non-zero but unpredictable speed, and single‐word load/store instructions are atomic.\nTwo-Process Software Algorithms Let the processes be P1 and P1. turn is an integer (0 or 1); flag[2] is a Boolean array.\nAlgorithm 1 – Strict Alternation // shared: int turn = 0; // 0 → P1’s turn, 1 → P1’s turn while (true) { while (turn != i) ; // entry section (busy-wait) /* critical section */ turn = j; // give turn to the other process /* remainder section */ } Property Result Why? Mutual Exclusion ✅ Only one turn value exists. Progress ❌ If turn == 0 and P1 is ready while P1 is in its remainder section, P1 still spins. Bounded Waiting ✅ Processes alternate strictly. Algorithm 2 – Individual Flags // shared: bool flag[2] = {false,false}; // code for Pi (j = 1 - i) while (true) { flag[i] = true; // I want to enter while (flag[j]) ; // wait while the other is ready /* critical section */ flag[i] = false; // I’m done /* remainder section */ } Property Result Why? Mutual Exclusion ✅ A process enters only if flag[j] == false. Progress ❌ If P1 sets flag[0]=true and, immediately after, P1 sets flag[1]=true, both spin forever. Bounded Waiting ❌ Same scenario causes indefinite waiting. Algorithm 3 – Peterson’s Algorithm // shared: bool flag[2] = {false,false}; int turn = 0; // whose turn if conflict // code for Pi (j = 1 - i) while (true) { flag[i] = true; // I want to enter turn = j; // let the other go first while (flag[j] \u0026\u0026 turn == j) ; // wait if j also wants in /* critical section */ flag[i] = false; // I’m done /* remainder section */ } Property Result Reasoning (sketch) Mutual Exclusion ✅ Both flag[j] and turn==j can’t be true for both processes simultaneously. Progress ✅ At most one process waits if both want to enter. Bounded Waiting ✅ A waiting process can be bypassed only once before entering. N-Process Solution – Bakery Algorithm Every process takes a “ticket” like customers at a bakery.\nShared Variables bool choosing[n] = {false}; // true while Pi is picking a number int number[n] = {0}; // ticket numbers (0 ⇒ not competing) Pi’s Code while (true) { // entry section choosing[i] = true; // announce intention number[i] = 1 + max(number[0..n-1]); // take next ticket choosing[i] = false; fo*r (int j = 0; j \u003c n; j++) { // wait for turn while (choosing[j]) ; // wait if Pj choosing while (number[j] != 0 \u0026\u0026 (number[j] \u003c number[i] || (number[j] == number[i] \u0026\u0026 j \u003c i))) ; } /* critical section */ number[i] = 0; // exit section /* remainder section */ } Why It Works Requirement Satisfied? Explanation Mutual Exclusion ✅ Two processes cannot both see themselves “smaller” in the same lexicographic (number, id) ordering. Progress ✅ Tickets strictly increase; comparison uses only currently contending processes. Bounded Waiting ✅ Each process waits behind a finite set of smaller ticket numbers. New tickets are always larger. The Bakery Algorithm generalizes Peterson’s idea to any number of processes without special hardware, guaranteeing a fair, starvation-free ordering of critical-section access.\nHere’s an example problem based on the Bakery Algorithm and the execution sequence you’ve described:\nExample Problem: Five processes P1 through P4 are competing for entry into their critical sections using Lamport’s Bakery Algorithm.\nAt a particular moment, the following occurs:\nAll five processes begin executing their entry sections at approximately the same time.\nP1 does not intend to enter its critical section, and therefore skips taking a ticket (number[1] = 0).\nAll other processes (i.e., P1, P2, P3, P4) enter the entry section and begin executing the choosing[i] and number[i] assignment part.\nThe resulting ticket numbers are:\nP1 gets ticket 1 P2 gets ticket 3 P3 gets ticket 2 P4 gets ticket 4 All processes now enter the second for loop of the Bakery Algorithm and start comparing their ticket numbers to others.\nPart A: Waiting Table (simplified version)\nProcess Waiting for Reason P1 None Has lowest ticket number (1) P2 P1, P3 P1 has lower ticket (1), P3 has lower ticket (2) P3 P1 P1 has lower ticket (1) P4 P1, P3, P2 All have lower ticket numbers (1, 2, 3) Part B: Execution Sequence\nP1 enters critical section first. P1 exits CS and sets number[0] = 0. Now P3 has the smallest active number (2) → enters CS. P3 exits and resets number[3] = 0. Now P2 has the next smallest ticket (3) → enters CS. P2 exits and resets number[2] = 0. P4 now has the smallest active ticket (4) → enters CS. Execution Order: \u003cP1, P3, P2, P4\u003e","problem-statement#Problem Statement":"","two-process-software-algorithms#Two-Process Software Algorithms":""},"title":"Critical Section Problem"},"/process-synchronization/dining-philosipher-problem/":{"data":{"":"","correctness-achieved#Correctness Achieved":" Requirement Mechanism Mutual Exclusion Global mutex ensures exclusive access to shared state. Deadlock-Free test() only allows eating if neighbors are not eating. No Starvation Each philosopher eventually gets a chance to eat via FIFO semaphore S[i]. Concurrency Multiple non-neighbor philosophers can eat simultaneously. ","correctness-issues#Correctness Issues":" Requirement Problem in Naive Solution Mutual Exclusion Satisfied via binary semaphores on forks. Deadlock-Free ❌ May deadlock if all pick left fork first. No Starvation ❌ If a philosopher is always blocked by others. Concurrency ✅ When forks are free, philosophers eat freely. ","deadlock#Deadlock":"If all philosophers pick up their left forks simultaneously, no one can pick up their right fork — resulting in a deadlock.\nCondition Present Mutual Exclusion Yes Hold and Wait Yes No Preemption Yes Circular Wait Yes All four Coffman conditions for deadlock are present.","how-the-semaphores-coordinate-the-processes#How the Semaphores Coordinate the Processes":" Step Semaphore Action Effect Acquire forks wait(fork[i]) and wait(fork[(i+1)%N]) Philosopher picks up both forks or blocks if unavailable. Eat section No conflict Exclusive access to both forks ensures mutual exclusion. Release forks signal(fork[i]) and signal(fork[(i+1)%N]) Makes forks available to neighbors. ","how-the-semaphores-coordinate-the-processes-1#How the Semaphores Coordinate the Processes":" Issue Resolution Deadlock Avoided: a philosopher only proceeds when both forks are available. Starvation Prevented by checking neighbors after every put_forks (fair wakeups). Mutual Exclusion Guaranteed: Only one philosopher accesses shared state at a time. Concurrency Enabled: Non-adjacent philosophers may eat simultaneously. ","problem-statement#Problem Statement":"Problem Statement Five philosophers sit around a circular table. Each philosopher alternates between two states:\nState Action Thinking Does not interact with forks. Eating Needs two forks (left and right) to eat. Each philosopher must pick up their left and right forks to eat. Each fork is shared between adjacent philosophers.","solution-improved#Solution (Improved)":"To avoid deadlock and ensure progress and bounded waiting, the solution must prevent all four philosophers from holding one fork and waiting for the other.\nUse a single mutex semaphore for mutual exclusion and N state flags to track each philosopher’s status. Use condition semaphores for each philosopher to avoid busy waiting.\nSynchronization Primitives State Meaning THINKING Not interested in forks HUNGRY Wants to eat (tries to pick forks) EATING Holding both forks Shared Data #define N 5 enum { THINKING, HUNGRY, EATING } state[N]; // philosopher states semaphore mutex = 1; // global mutex semaphore S[N]; // one semaphore per philosopher test(i) Check if philosopher i can eat (both neighbors are not eating).\nvoid test(int i) { if (state[i] == HUNGRY \u0026\u0026 state[(i+N-1)%N] != EATING \u0026\u0026 state[(i+1)%N] != EATING) { state[i] = EATING; signal(S[i]); // wake up philosopher i } } take_forks(i) Philosopher i wants to eat.\nvoid take_forks(int i) { wait(mutex); // enter critical section state[i] = HUNGRY; test(i); // check if i can eat signal(mutex); // leave critical section wait(S[i]); // block if not yet able to eat } put_forks(i) Philosopher i finishes eating and releases forks.\nvoid put_forks(int i) { wait(mutex); // enter critical section state[i] = THINKING; test((i+N-1)%N); // check if left neighbor can now eat test((i+1)%N); // check if right neighbor can now eat signal(mutex); // leave critical section } Philosopher Code while (true) { think(); // not interested in forks take_forks(i); // try to acquire forks eat(); // critical section put_forks(i); // release forks } ","solution-naive#Solution (Naive)":"Synchronization Primitives Semaphore Initial Value Purpose fork[i] 1 Represents availability of fork i. Only one philosopher can hold it at a time. Shared Data #define N 5 // number of philosophers semaphore fork[N] = {1,1,1,1,1}; // one semaphore per fork Philosopher Process while (true) { think(); // not accessing forks wait(fork[i]); // pick up left fork wait(fork[(i+1) % N]); // pick up right fork eat(); // critical section signal(fork[i]); // put down left fork signal(fork[(i+1) % N]); // put down right fork } "},"title":"Dining Philosipher Problem"},"/process-synchronization/producer-consumer-problem/":{"data":{"":"","buffer-types#Buffer Types":" Buffer Type Capacity Producer Waits When Consumer Waits When Bounded Fixed N Buffer full Buffer empty Unbounded Unlimited (practically) ― (can always insert) Buffer empty In practice, most implementations use a bounded circular buffer because physical memory is finite.\nCertainly.","busy-waiting-implementation#Busy-Waiting Implementation":"Shared Data #define BUFFER_SIZE 10 item buffer[BUFFER_SIZE]; int in = 0, out = 0, counter = 0; buffer[BUFFER_SIZE]: The circular buffer that holds items produced by the producer and consumed by the consumer. in: Index where the next item will be placed by the producer. out: Index where the next item will be removed by the consumer. counter: Tracks the number of items currently in the buffer. Producer while (1) { item nextProduced = produce_item(); // create data while (counter == BUFFER_SIZE); // wait if buffer is full buffer[in] = nextProduced; // place item in buffer in = (in + 1) % BUFFER_SIZE; // move to next index counter++; // increment item count } produce_item(): Generates an item. The while(counter == BUFFER_SIZE); loop ensures the producer waits if the buffer is full. Once space is available, the item is inserted at the in index, in is updated modulo BUFFER_SIZE to wrap around circularly, and the counter is incremented. Consumer while (1) { while (counter == 0); // wait if buffer is empty item nextConsumed = buffer[out]; // take item from buffer out = (out + 1) % BUFFER_SIZE; // move to next index counter--; // decrement item count consume_item(nextConsumed); // use data } The while(counter == 0); loop causes the consumer to wait if the buffer is empty. Once an item is available, it’s taken from the out index, out is updated in a circular fashion, and counter is decremented. consume_item() consumes the retrieved item. Summary This implementation relies on a shared circular buffer and busy-wait loops for synchronization. It assumes the producer and consumer will not interfere with each other’s updates to counter, in, and out. In practice, accessing shared variables like counter without proper synchronization leads to race conditions. ","problem-statement#Problem Statement":"Problem Statement Two cooperating processes—Producer and Consumer—share a buffer:\nProducer generates items and inserts them into the buffer. Consumer removes and uses those items. ","race-condition#Race Condition":"Without proper synchronization, concurrent updates to shared variables (e.g., counter) can interleave, producing incorrect results. The system must guarantee mutually exclusive access to the buffer and accurate tracking of its current occupancy."},"title":"Producer Consumer Problem"},"/process-synchronization/reader-writer-problem/":{"data":{"":"","readers-writers-1#Readers Writers 1":"Readers Writers 1 Problem Statement Two kinds of concurrent processes share a common data set:\nRole Action Reader Needs read-only access; many readers may read simultaneously. Writer Needs exclusive access; no other reader or writer may access the data while a writer is active. Constraints:\nAny number of readers may read concurrently. A writer must have exclusive access—no reader or other writer can access the data during a write. Solution (Readers-Preference) Synchronization Primitives Semaphore Initial Value Meaning mutex 1 Binary semaphore protecting the shared variable readCount. wrt 1 Binary semaphore granting writers exclusive access to the data; first reader also waits on it. readCount (integer) counts active readers. All semaphore operations are atomic.\nShared Data int readCount = 0; // number of active readers semaphore mutex = 1; // guards readCount semaphore wrt = 1; // writers (and first/last reader) lock Reader while (true) { wait(mutex); // protect readCount readCount++; if (readCount == 1) // first reader locks writers out wait(wrt); signal(mutex); // release readCount lock /* ----- critical section (read) ----- */ read_data(); /* ----------------------------------- */ wait(mutex); // protect readCount readCount--; if (readCount == 0) // last reader lets writers in signal(wrt); signal(mutex); // release readCount lock remainder_section(); } Writer while (true) { wait(wrt); // request exclusive access /* ----- critical section (write) ----- */ write_data(); /* ------------------------------------ */ signal(wrt); // release exclusive access remainder_section(); } How the Semaphores Coordinate the Processes Step Semaphore Action Effect First reader entry wait(mutex) → readCount++ → wait(wrt) Locks out writers; subsequent readers proceed without waiting on wrt. Additional readers wait(mutex) / signal(mutex) Only update readCount; no writer exclusion needed if readCount \u003e 0. Last reader exit wait(mutex) → readCount-- → signal(wrt) Re-enables writers when no reader remains. Writer entry / exit wait(wrt) / signal(wrt) Provides writers with exclusive access; blocks readers when held. Correctness Achieved Requirement Mechanism Mutual Exclusion wrt held by either a writer or the reader group (via first reader) ensures no overlap. No Read–Write Conflict Writers cannot enter while wrt is held by readers; readers cannot start if a writer holds wrt. Reader Concurrency Multiple readers skip wrt once the first reader has locked it, allowing parallel reads. Progress If the data is free, whichever process (reader or writer) obtains the relevant semaphore first proceeds. Bounded Waiting Readers waiting only contend for mutex; writers wait at most until current readers finish. This semaphore arrangement (readers-preference variant) maximizes reader concurrency while still guaranteeing writers exclusive access when required.","readers-writers-2#Readers Writers 2":"This is the second classical formulation of the Readers-Writers problem — known as the Writers-Preference solution.\nProblem Statement Two types of concurrent processes — readers and writers — share a common resource (usually a database or file):\nRole Goal Readers May access the resource simultaneously with other readers. Writers Must have exclusive access — no other readers or writers allowed. Solution (Writers-Preference) Synchronization Primitives Semaphore Initial Value Purpose mutex1 1 Ensures mutual exclusion for readCount. mutex2 1 Ensures mutual exclusion for writeCount. readTry 1 Allows writers to block new readers when they are waiting. wrt 1 Actual access control to the shared resource (held exclusively). Shared Data int readCount = 0; // number of active readers int writeCount = 0; // number of waiting writers semaphore mutex1 = 1; // protects readCount semaphore mutex2 = 1; // protects writeCount semaphore readTry = 1; // blocks readers if writer is waiting semaphore wrt = 1; // resource access Reader Process while (true) { wait(readTry); // Check if writers are waiting wait(mutex1); readCount++; if (readCount == 1) // First reader locks the resource wait(wrt); signal(mutex1); signal(readTry); // ----------- Critical Section (Read) ----------- read_data(); // ----------------------------------------------- wait(mutex1); readCount--; if (readCount == 0) // Last reader releases the resource signal(wrt); signal(mutex1); remainder_section(); } Writer Process while (true) { wait(mutex2); writeCount++; if (writeCount == 1) // First writer blocks new readers wait(readTry); signal(mutex2); wait(wrt); // Lock the resource // ----------- Critical Section (Write) ---------- write_data(); // ----------------------------------------------- signal(wrt); wait(mutex2); writeCount--; if (writeCount == 0) // Last writer allows readers again signal(readTry); signal(mutex2); remainder_section(); } How the Semaphores Coordinate the Processes Step Semaphore Mechanism Effect Writers signal intent First writer locks readTry Prevents new readers from entering if any writer is waiting. Multiple writers wait All writers still wait on wrt Still enforces one-writer-at-a-time. First reader waits Blocks on readTry if writer is pending Enforces writer preference. Last reader unlocks readCount == 0 → signal(wrt) Allows writers to proceed once all readers finish. Last writer unlocks readers writeCount == 0 → signal(readTry) New readers are now allowed to proceed. Correctness Achieved Requirement Mechanism Mutual Exclusion wrt guarantees only one writer or many readers access the resource at any one time. Reader Concurrency Multiple readers are allowed when no writer is waiting or active. Writer Preference readTry blocks new readers once a writer is waiting, giving writers priority. No Starvation Writers get access once all active readers finish and new ones are blocked. Bounded Waiting Writers waiting first will proceed first (FIFO on wrt), and readers are blocked until done. This version avoids starvation of writers by preventing new readers from entering the critical section once a writer has requested access. It ensures fairness toward writers even in a heavily read-oriented system."},"title":"Readers Writers Problem"},"/process-synchronization/semaphores/":{"data":{"":"","binary-vs-counting-semaphores#Binary vs Counting Semaphores":" Type Initial Range Meaning of Value Main Purpose Analogy Binary Semaphore 0 or 1 1 ⇒ resource free, 0 ⇒ resource busy Acts like a mutex lock for a single shared resource or critical section. Light switch: on or off. Counting Semaphore Any non-negative integer Value = number of identical resource units available Controls access to a pool of interchangeable resources (buffer slots, database connections, etc.). Parking lot counter: slots left. Binary Semaphore Characteristics Ensures mutual exclusion for one resource. Only the first caller after a signal proceeds; others must wait. Counting Semaphore Characteristics Allows up to N concurrent accesses where N is the initial value. Becomes binary when initialized to 1. ","concept#Concept":"Concept A semaphore is a kernel-managed integer variable used to coordinate access to shared resources among concurrent processes or threads. Two indivisible operations manipulate its value:\nOperation Conceptual Action wait (P) Decrease the semaphore value. If it becomes negative, the caller is suspended. signal (V) Increase the semaphore value. If the new value is non-positive, one suspended caller is resumed. Atomicity of each operation prevents race conditions on the semaphore itself.","functional-roles#Functional Roles":" Responsibility How a Semaphore Fulfills It Mutual Exclusion A binary semaphore guards entry and exit of a critical section. Resource Counting A counting semaphore tracks how many identical resources remain; callers wait when none are free. Producer–Consumer Synchronization Two counting semaphores (empty, full) plus one binary semaphore (mutex) coordinate buffer space and mutual exclusion. Ordering / Signaling A semaphore initialized to 0 can force one thread to pause until another issues a signal, establishing sequence constraints. Semaphores thus provide a foundational, hardware-independent mechanism for coordinating concurrent activities in operating systems and multithreaded programs.","implementation-styles#Implementation Styles":" Style Core Idea When a Process Must Wait CPU Cost While Waiting Typical Use Case Spinlock (busy-waiting) Caller repeatedly tests the semaphore in a tight loop. Spins inside user code or a short kernel loop. Consumes CPU cycles. Ultra-short critical sections on multiprocessors where a context-switch would cost more than the spin. Blocking (queue-based) Caller is placed on a kernel queue linked to the semaphore. Kernel deschedules the caller (WAITING state). No CPU cycles consumed; context switch cost instead. Longer critical sections or uniprocessor systems where CPU time must be preserved. Spinlocks favor latency; blocking semaphores favor overall CPU efficiency."},"title":"Semaphores"},"/process-synchronization/shared-memory/":{"data":{"":"","concept#Concept":"Concept Shared memory is an inter-process communication (IPC) mechanism where multiple processes share a region of memory. This region is created by one process using shmget() and then mapped into the address space of other processes via shmat().\nThe OS typically isolates memory between processes. However, shared memory explicitly bypasses this restriction by mutual agreement between processes. Once attached, processes can directly read from and write to the shared region. This memory is not controlled or interpreted by the OS—the processes using it define the format, meaning, and structure of the data. Synchronization is critical: the OS does not prevent race conditions. It’s up to the processes to ensure safe access—typically by using semaphores or mutexes. This makes shared memory very fast for IPC, since there’s no copying of data between processes, but it also requires careful coordination.","race-condition#Race Condition":"A race condition occurs when the outcome of a program depends on the timing or sequence of uncontrollable events like context switches between threads or processes. It happens when:\nTwo or more threads/processes access shared data simultaneously. At least one of them is modifying the data. There is no proper synchronization to control the access. This can lead to unexpected behavior, data corruption, or crashes.","race-condition-scenario#Race Condition Scenario":" sequenceDiagram participant Thread A participant Shared Memory participant Thread B Thread A-\u003e\u003eShared Memory: Read value (x = 5) Thread B-\u003e\u003eShared Memory: Read value (x = 5) Thread A-\u003e\u003eThread A: Increment x (x = 6) Thread B-\u003e\u003eThread B: Increment x (x = 6) Thread A-\u003e\u003eShared Memory: Write x = 6 Thread B-\u003e\u003eShared Memory: Write x = 6 Note over Shared Memory: Final value 7 but is 6 (race condition) In the above diagram, both threads read the same initial value and update it without knowing the other thread’s action. The final result does not reflect both increments, demonstrating a race condition."},"title":"Shared Memory"},"/process-synchronization/the-sleeping-barber-problem/":{"data":{"":"","correctness-achieved#Correctness Achieved":" Requirement Mechanism Mutual Exclusion mutex protects access to waiting variable. Deadlock-Free Semaphores coordinate process entry without circular waits. No Starvation FIFO nature of semaphore queues ensures fair access. Efficiency Barber only sleeps when no customers; customers only wait if space is available. This model ensures that both customers and barbers behave correctly and efficiently with respect to the limited resources in the barbershop.","how-the-semaphores-coordinate-the-processes#How the Semaphores Coordinate the Processes":" Step Semaphore Used Effect Customer arrives wait(mutex) Enters critical section to check space in waiting room Free chair available waiting++ Sits and waits Notify barber signal(customers) Increments count of waiting customers Barber sleeps wait(customers) Waits for a customer to arrive Barber wakes up signal(barbers) Wakes up and allows customer to get haircut Protect waiting counter mutex Ensures waiting is updated atomically ","problem-statement#Problem Statement":"Problem Statement The Sleeping Barber Problem is a classic inter-process communication and synchronization problem. It models a barbershop with the following characteristics:\nElement Description Barber Sleeps when there are no customers. Wakes up and serves a customer if there is one. Customers Either get a haircut or leave if the waiting room is full. Waiting Room Has a limited number of chairs. Key Conditions ","solution#Solution":" If there are no customers, the barber sleeps.\nWhen a customer arrives:\nIf all chairs are occupied, the customer leaves. If there is a free chair, the customer sits and waits. When the barber finishes a haircut:\nHe checks for waiting customers. If no one is waiting, he sleeps again. Synchronization Primitives Semaphore Initial Value Purpose customers 0 Counts waiting customers (to wake up barber). barbers 0 Number of barbers ready to cut hair. mutex 1 Ensures mutual exclusion for access to waiting. Shared Variables int waiting = 0; // number of customers waiting int CHAIRS = N; // total number of chairs semaphore customers = 0; // customers waiting semaphore barbers = 0; // barbers ready semaphore mutex = 1; // for mutual exclusion Customer Process Customer() { wait(mutex); // enter critical section if (waiting \u003c CHAIRS) { waiting++; // sit in a waiting chair signal(customers); // notify barber signal(mutex); // leave critical section wait(barbers); // wait to be called by barber get_haircut(); } else { signal(mutex); // leave (no chair available) leave_shop(); } } Barber Process Barber() { while (true) { wait(customers); // sleep if no customers wait(mutex); // enter critical section waiting--; // take a customer out of waiting signal(barbers); // call customer signal(mutex); // leave critical section cut_hair(); // perform haircut } } "},"title":"Sleeping Barber Problem"},"/semaphores/":{"data":{"":"","library-required#Library Required":"Library Required #include \u003csemaphore.h\u003e ","sem_close#sem_close()":"Closes a named semaphore descriptor without removing the semaphore.\nint sem_close(sem_t *sem); Return Value:\nSuccess: 0 Error: -1 Example:\nsem_close(sem); ","sem_destroy#sem_destroy()":"Destroys an unnamed semaphore, freeing associated resources.\nint sem_destroy(sem_t *sem); Return Value:\nSuccess: 0 Error: -1 Example:\nsem_destroy(\u0026sem_local); ","sem_getvalue#sem_getvalue()":"Retrieves the current value of the semaphore.\nint sem_getvalue(sem_t *sem, int *sval); Return Value:\nSuccess: 0 Error: -1 Example:\nint val; sem_getvalue(sem, \u0026val); ","sem_init#sem_init()":"Initializes an unnamed semaphore for use within a process or between processes.\nint sem_init(sem_t *sem, int pshared, unsigned int value); Return Value:\nSuccess: 0 Error: -1 Example:\nsem_t sem_local; sem_init(\u0026sem_local, 0, 1); ","sem_open#sem_open()":"Opens or creates a named semaphore and returns a pointer to it.\nsem_t *sem_open(const char *name, int oflag, mode_t mode, unsigned int value); Return Value:\nSuccess: Pointer to semaphore Error: SEM_FAILED Example:\nsem_t *sem = sem_open(\"/mysem\", O_CREAT | O_EXCL, 0644, 2); ","sem_post#sem_post()":"Increments (unlocks) the semaphore, potentially waking a waiting thread.\nint sem_post(sem_t *sem); Return Value:\nSuccess: 0 Error: -1 Example:\nsem_post(sem); ","sem_trywait#sem_trywait()":"Tries to decrement the semaphore without blocking; fails if the value is zero.\nint sem_trywait(sem_t *sem); Return Value:\nSuccess: 0 Error: -1 Example:\nif (sem_trywait(sem) == -1) { // handle failure without blocking } ","sem_unlink#sem_unlink()":"Removes a named semaphore from the system\nint sem_unlink(const char *name); Return Value:\nSuccess: 0 Error: -1 Example:\nsem_unlink(\"/mysem\"); ","sem_wait#sem_wait()":"Decrements (locks) the semaphore, blocking if its value is zero.\nint sem_wait(sem_t *sem); Return Value:\nSuccess: 0 Error: -1 Example:\nsem_wait(sem); ","types#Types":" sem_t // unique identifier for a semaphore "},"title":"Semaphore"},"/shared-memory/":{"data":{"":"","libraries-required#Libraries Required":"Libraries Required #include \u003csys/ipc.h\u003e #include \u003csys/shm.h\u003e #include \u003csys/sem.h\u003e #include \u003csys/mman.h\u003e #include \u003cfcntl.h\u003e #include \u003cunistd.h\u003e ","memory-mapping#Memory Mapping":"mmap() Maps a file or anonymous memory into the process’s address space\nvoid *mmap(void *addr, size_t length, int prot, int flags, int fd, off_t offset); Return Value:\nSuccess: Pointer to mapped area Error: MAP_FAILED Example:\nint fd = open(\"file.txt\", O_RDWR); char *addr = mmap(NULL, 4096, PROT_READ | PROT_WRITE, MAP_SHARED, fd, 0); Options Flag / Protection Description PROT_READ Pages can be read PROT_WRITE Pages can be written MAP_SHARED Updates visible to other processes mapping same region MAP_PRIVATE Copy-on-write; changes not visible to other processes MAP_ANONYMOUS Mapping is not backed by any file (used with -1 as fd) MAP_FAILED Return value on error munmap() Unmaps a previously mapped memory region from the address space\nint munmap(void *addr, size_t length); Return Value:\nSuccess: 0 Error: -1 Example:\nmunmap(addr, 4096); ","semget#semget()":"Creates a new semaphore set or accesses an existing one\nint semget(key_t key, int nsems, int semflg); Return Value:\nSuccess: Semaphore set ID Error: -1 Example:\nint semid = semget(IPC_PRIVATE, 1, IPC_CREAT | 0666); Options Option Description IPC_CREAT Create the semaphore set if it does not exist IPC_EXCL Fail if it exists (used with IPC_CREAT) 0666 Read \u0026 write permissions ","semop#semop()":"Performs one or more atomic operations on semaphores\nint semop(int semid, struct sembuf *sops, size_t nsops); Return Value:\nSuccess: 0 Error: -1 Example:\nsops[0].sem_num = 0; sops[0].sem_op = -1; sops[0].sem_flg = 0; semop(semid, sops, 1); ","shmat#shmat()":"Attaches a shared memory segment to the process’s address space\nvoid *shmat(int shmid, const void *shmaddr, int shmflg); Return Value:\nSuccess: Pointer to shared memory segment Error: -1 Example:\nchar *data = (char *)shmat(shmid, NULL, 0); ","shmctl#shmctl()":"Performs control operations on a shared memory segment (eg., remove, get info).\nint shmctl(int shmid, int cmd, struct shmid_ds *buf); Return Value:\nSuccess: 0 Error: -1 Example:\nshmctl(shmid, IPC_RMID, NULL); Options Command Description IPC_STAT Get current segment status into shmid_ds struct IPC_SET Set permissions and other fields from shmid_ds IPC_RMID Remove the segment ","shmdt#shmdt()":"Detaches a shared memory segment from the process’s address space\nint shmdt(const void *shmaddr); Return Value:\nSuccess: 0 Error: -1 Example:\nshmdt(data); ","shmget#shmget()":"Allocates or accesses a shared memory segment using a key\nint shmget(key_t key, size_t size, int shmflg); Return Value:\nSuccess: Shared memory segment ID Error: -1 Example:\nint shmid = shmget(IPC_PRIVATE, 1024, IPC_CREAT | 0666); Options Option Description IPC_CREAT Create the segment if it does not exist IPC_EXCL Fail if segment exists (used with IPC_CREAT) 0666 Read \u0026 write permissions for all (standard mode) "},"title":"Shared Memory"},"/threading/":{"data":{"":"","library-required#Library Required":"","pthread_attr_destroy#pthread_attr_destroy()":"Library Required #include \u003cpthread.h\u003e Types pthread_t // uniquely identify a thread in POSIX thread programming pthread_attr_t // uniquely identify a thread attributes in POSIX pthread_create() Creates a new thread and starts executing the specified routine in parallel\nint pthread_create(pthread_t *thread, const pthread_attr_t *attr, void *(*start_routine)(void *), void *arg); Return Value:\nSuccess: 0 Error: Error number Example:\nvoid* print_msg(void* arg) { printf(\"Thread: %s\\n\", (char*)arg); return NULL; } pthread_t tid; pthread_create(\u0026tid, NULL, print_msg, \"Hello from thread\"); pthread_join() Waits for the specified thread to terminate and optionally collects its return value\nint pthread_join(pthread_t thread, void **retval); Return Value:\nSuccess: 0 Error: Error number Example:\nvoid* thread_func(void* arg) { return (void*)42; } pthread_t tid; pthread_create(\u0026tid, NULL, thread_func, NULL); void* retval; pthread_join(tid, \u0026retval); printf(\"Thread returned: %ld\\n\", (long)retval); pthread_detach() Marks a thread as detached so its resources are automatically released on termination\nint pthread_detach(pthread_t thread); Return Value:\nSuccess: 0 Error: Error number Example:\nvoid* detached_func(void* arg) { pthread_exit(NULL); } pthread_t tid; pthread_create(\u0026tid, NULL, detached_func, NULL); pthread_detach(tid); pthread_exit() Terminates the calling thread and optionally returns a value to any joining thread\nvoid pthread_exit(void *retval); Return Value:\nDoes not return Example:\nvoid* thread_func(void* arg) { pthread_exit(\"Finished\"); return NULL; } pthread_t tid; pthread_create(\u0026tid, NULL, thread_func, NULL); pthread_self() Returns the thread ID of the calling thread\npthread_t pthread_self(void); Return Value:\nThe ID of the calling thread Example:\nvoid* thread_func(void* arg) { printf(\"Thread ID: %lu\\n\", pthread_self()); return NULL; } pthread_t tid; pthread_create(\u0026tid, NULL, thread_func, NULL); pthread_attr_init() Initializes a thread attribute object with default values\nint pthread_attr_init(pthread_attr_t *attr); Return Value:\nSuccess: 0 Error: Error number Example:\npthread_attr_t attr; pthread_attr_init(\u0026attr); pthread_attr_destroy() Destroys a thread attribute object and frees associated resources\nint pthread_attr_destroy(pthread_attr_t *attr); Return Value:\nSuccess: 0 Error: Error number Example:\npthread_attr_t attr; pthread_attr_init(\u0026attr); pthread_attr_destroy(\u0026attr); ","pthread_attr_init#pthread_attr_init()":"","pthread_create#pthread_create()":"","pthread_detach#pthread_detach()":"","pthread_exit#pthread_exit()":"","pthread_join#pthread_join()":"","pthread_self#pthread_self()":"","thread-attr-settergetter#Thread Attr Setter/Getter":"These functions are used to set and get properties of a pthread_attr_t object before creating a thread.\npthread_attr_setdetachstate / pthread_attr_getdetachstate Sets or gets the detach state (joinable or detached).\nint pthread_attr_setdetachstate(pthread_attr_t *attr, int detachstate); int pthread_attr_getdetachstate(const pthread_attr_t *attr, int *detachstate); Values: PTHREAD_CREATE_JOINABLE (default) · PTHREAD_CREATE_DETACHED\nExample\npthread_attr_t attr; pthread_attr_init(\u0026attr); pthread_attr_setdetachstate(\u0026attr, PTHREAD_CREATE_DETACHED); int state; pthread_attr_getdetachstate(\u0026attr, \u0026state); // state now holds the detach flag pthread_attr_destroy(\u0026attr); pthread_attr_setschedpolicy / pthread_attr_getschedpolicy Sets or gets the scheduling policy.\nint pthread_attr_setschedpolicy(pthread_attr_t *attr, int policy); int pthread_attr_getschedpolicy(const pthread_attr_t *attr, int *policy); Policies: SCHED_OTHER (default) · SCHED_FIFO · SCHED_RR\nExample\npthread_attr_t attr; pthread_attr_init(\u0026attr); pthread_attr_setschedpolicy(\u0026attr, SCHED_FIFO); int policy; pthread_attr_getschedpolicy(\u0026attr, \u0026policy); // policy now SCHED_FIFO pthread_attr_destroy(\u0026attr); pthread_attr_setschedparam / pthread_attr_getschedparam Sets or gets scheduling parameters (e.g., priority).\nint pthread_attr_setschedparam(pthread_attr_t *attr, const struct sched_param *param); int pthread_attr_getschedparam(const pthread_attr_t *attr, struct sched_param *param); Example\npthread_attr_t attr; pthread_attr_init(\u0026attr); struct sched_param sp = { .sched_priority = 20 }; pthread_attr_setschedparam(\u0026attr, \u0026sp); struct sched_param out; pthread_attr_getschedparam(\u0026attr, \u0026out); // out.sched_priority == 20 pthread_attr_destroy(\u0026attr); pthread_attr_setinheritsched / pthread_attr_getinheritsched Sets or gets whether the thread inherits or explicitly uses scheduling attributes.\nint pthread_attr_setinheritsched(pthread_attr_t *attr, int inherit); int pthread_attr_getinheritsched(const pthread_attr_t *attr, int *inherit); Values: PTHREAD_INHERIT_SCHED · PTHREAD_EXPLICIT_SCHED\nExample\npthread_attr_t attr; pthread_attr_init(\u0026attr); pthread_attr_setinheritsched(\u0026attr, PTHREAD_EXPLICIT_SCHED); int inherit; pthread_attr_getinheritsched(\u0026attr, \u0026inherit); // inherit now explicit pthread_attr_destroy(\u0026attr); pthread_attr_setstacksize / pthread_attr_getstacksize Sets or gets the thread stack size.\nint pthread_attr_setstacksize(pthread_attr_t *attr, size_t stacksize); int pthread_attr_getstacksize(const pthread_attr_t *attr, size_t *stacksize); Example\npthread_attr_t attr; pthread_attr_init(\u0026attr); pthread_attr_setstacksize(\u0026attr, 1024 * 1024); // 1 MB size_t sz; pthread_attr_getstacksize(\u0026attr, \u0026sz); // sz == 1048576 pthread_attr_destroy(\u0026attr); pthread_attr_setstack / pthread_attr_getstack Sets or gets both stack address and size.\nint pthread_attr_setstack(pthread_attr_t *attr, void *stackaddr, size_t stacksize); int pthread_attr_getstack(const pthread_attr_t *attr, void **stackaddr, size_t *stacksize); Example\nchar stack_area[64 * 1024]; // 64 KB buffer pthread_attr_t attr; pthread_attr_init(\u0026attr); pthread_attr_setstack(\u0026attr, stack_area, sizeof stack_area); void *addr; size_t sz; pthread_attr_getstack(\u0026attr, \u0026addr, \u0026sz); // addr == stack_area, sz == 65536 pthread_attr_destroy(\u0026attr); ","types#Types":""},"title":"Threading"},"/virtual-memory/page-replacement-algorithm/":{"data":{"":" FIFO Optimal Page Replacement Least Recently Used Clock "},"title":"Page Repalcement Algorithm"}}